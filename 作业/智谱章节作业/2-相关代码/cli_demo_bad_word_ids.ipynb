{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ab6971e-5276-4283-b66b-8ad3c9f39f67",
   "metadata": {},
   "source": [
    "# cli_demo_bad_word_ids.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c3141f-7a81-4420-a7bf-5db6d228250a",
   "metadata": {},
   "source": [
    "# 整合前测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70729208-4f81-47c6-b51f-8b031ffdc31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa73c41f-e24b-476e-8f5a-6e77ccf09b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12195720\n",
      "-rw------- 1 root       4133 Apr  3 01:14 MODEL_LICENSE\n",
      "-rw------- 1 root       4478 Apr  3 01:19 README.md\n",
      "-rw------- 1 root       1317 Apr  3 01:14 config.json\n",
      "-rw------- 1 root         37 Apr  3 01:14 configuration.json\n",
      "-rw------- 1 root       2332 Apr  3 01:14 configuration_chatglm.py\n",
      "-rw------- 1 root      55596 Apr  3 01:14 modeling_chatglm.py\n",
      "-rw------- 1 root 1827781090 Apr  3 01:15 pytorch_model-00001-of-00007.bin\n",
      "-rw------- 1 root 1968299480 Apr  3 01:16 pytorch_model-00002-of-00007.bin\n",
      "-rw------- 1 root 1927415036 Apr  3 01:17 pytorch_model-00003-of-00007.bin\n",
      "-rw------- 1 root 1815225998 Apr  3 01:17 pytorch_model-00004-of-00007.bin\n",
      "-rw------- 1 root 1968299544 Apr  3 01:18 pytorch_model-00005-of-00007.bin\n",
      "-rw------- 1 root 1927415036 Apr  3 01:19 pytorch_model-00006-of-00007.bin\n",
      "-rw------- 1 root 1052808542 Apr  3 01:19 pytorch_model-00007-of-00007.bin\n",
      "-rw------- 1 root      20437 Apr  3 01:19 pytorch_model.bin.index.json\n",
      "-rw------- 1 root      14692 Apr  3 01:19 quantization.py\n",
      "-rw------- 1 root      11279 Apr  3 01:19 tokenization_chatglm.py\n",
      "-rw------- 1 root    1018370 Apr  3 01:19 tokenizer.model\n",
      "-rw------- 1 root        244 Apr  3 01:19 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "ll /root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af09b39d-7b34-4020-acf7-533e4d830b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '/root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b'\n",
    "TOKENIZER_PATH = '/root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "963b03d0-8d4a-48a6-a075-38709826278d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8054837133f24d069be87a05fd12bc2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True, device_map=\"auto\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7ec4466-8249-49db-9eb2-fe9a8575d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os_name = platform.system()\n",
    "clear_command = 'cls' if os_name == 'Windows' else 'clear'\n",
    "stop_stream = False\n",
    "\n",
    "welcome_prompt = \"欢迎使用 ChatGLM3-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "708a84fe-fedb-4598-9f82-26a105751b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability tensor contains either `inf`, `nan` or element < 0\n",
    "\n",
    "bad_words = [\"你好\", \"ChatGLM\"]\n",
    "bad_word_ids = [tokenizer.encode(bad_word, add_special_tokens=False) for bad_word in bad_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9264bd7-b13f-4a74-9787-4b8a6ac4ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(history):\n",
    "    prompt = welcome_prompt\n",
    "    for query, response in history:\n",
    "        prompt += f\"\\n\\n用户：{query}\"\n",
    "        prompt += f\"\\n\\nChatGLM3-6B：{response}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2b80f96-6e27-4ae7-8acd-fa513635af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    past_key_values, history = None, []\n",
    "    global stop_stream\n",
    "    print(welcome_prompt)\n",
    "    while True:\n",
    "        query = input(\"\\n用户：\")\n",
    "        if query.strip().lower() == \"stop\":\n",
    "            break\n",
    "        if query.strip().lower() == \"clear\":\n",
    "            past_key_values, history = None, []\n",
    "            os.system(clear_command)\n",
    "            print(welcome_prompt)\n",
    "            continue\n",
    "\n",
    "        # Attempt to generate a response\n",
    "        try:\n",
    "            print(\"\\nChatGLM：\", end=\"\")\n",
    "            current_length = 0\n",
    "            response_generated = False\n",
    "            for response, history, past_key_values in model.stream_chat(\n",
    "                tokenizer, query, history=history, top_p=1,\n",
    "                temperature=0.01,\n",
    "                past_key_values=past_key_values,\n",
    "                return_past_key_values=True,\n",
    "                bad_words_ids=bad_word_ids  # assuming this is implemented correctly\n",
    "            ):\n",
    "                response_generated = True\n",
    "                # Check if the response contains any bad words\n",
    "                if any(bad_word in response for bad_word in bad_words):\n",
    "                    print(\"我的回答涉嫌了 bad word\")\n",
    "                    break  # Break the loop if a bad word is detected\n",
    "\n",
    "                # Otherwise, print the generated response\n",
    "                print(response[current_length:], end=\"\", flush=True)\n",
    "                current_length = len(response)\n",
    "            if not response_generated:\n",
    "                print(\"没有生成任何回答。\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"生成文本时发生错误：{e}，这可能是涉及到设定的敏感词汇\")\n",
    "\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2900bd24-9965-48bc-8d5f-fa2bb5498006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欢迎使用 ChatGLM3-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： 什么是badword,列举一些，我用来测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ChatGLM：\"badword\"是指在自然语言处理中，被认为是不合适或具有负面含义的词汇。这些词汇可能会被用于垃圾邮件、网络聊天、文本分析等场景中，可能会对其他用户造成负面影响。以下是一些常见的badword示例：\n",
      "\n",
      "1. 垃圾邮件中的常用badword：例如 \"free\", \"cash\", \"prize\", \"gift\", \"money\", \"offer\", \"win\", \"free money\", \"credit card\", \"loan\" 等。\n",
      "2. 社交媒体上的badword：例如 \"spam\", \"scam\", \"fake\", \"scandal\", \"hacker\", \"cyber attack\", \"phishing\", \" identity theft\", \"data breach\" 等。\n",
      "3. 在文本分析中常见的badword：例如 \"the\", \"and\", \"a\", \"an\", \"in\", \"to\", \"of\", \"on\", \"that\", \"this\", \"for\", \"with\", \"as\" 等。\n",
      "\n",
      "请注意，不同场景中的badword可能会有所不同，而且有些词汇在某些情况下可能并不被认为是badword。因此，在测试时，最好根据具体场景和目的来确定测试的badword列表。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： stop\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a90b818b-c4c1-49b2-94f2-7a6e8c3e2713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欢迎使用 ChatGLM3-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： 你好\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ChatGLM：生成文本时发生错误：probability tensor contains either `inf`, `nan` or element < 0，这可能是涉及到设定的敏感词汇\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： ChatGLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ChatGLM：生成文本时发生错误：probability tensor contains either `inf`, `nan` or element < 0，这可能是涉及到设定的敏感词汇\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： stop\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c89005-a215-407f-92d1-e47933520179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ac470-0e9c-4358-8326-7a99e0ea9742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e74ca3-3953-42f4-82c2-36823bf5d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整合后的cli_demo_bad_word_ids.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "293bb9fb-94d2-4b03-a7ba-c851244edde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "from pathlib import Path\n",
    "from typing import Annotated, Union, Optional\n",
    "from peft import AutoPeftModelForCausalLM, PeftModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerFast,\n",
    "    AutoModel,\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "\n",
    "ModelType = Union[PreTrainedModel, PeftModelForCausalLM]\n",
    "TokenizerType = Union[PreTrainedTokenizer, PreTrainedTokenizerFast]\n",
    "\n",
    "os_name = platform.system()\n",
    "clear_command = 'cls' if os_name == 'Windows' else 'clear'\n",
    "stop_stream = False\n",
    "\n",
    "welcome_prompt = \"欢迎使用 ChatGLM3-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\"\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(model_dir: Union[str, Path]) -> tuple[ModelType, TokenizerType]:\n",
    "    model_dir = _resolve_path(model_dir)\n",
    "    if (model_dir / 'adapter_config.json').exists():\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            model_dir, trust_remote_code=True, device_map='auto'\n",
    "        )\n",
    "        tokenizer_dir = model.peft_config['default'].base_model_name_or_path\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir, trust_remote_code=True, device_map='auto'\n",
    "        )\n",
    "        tokenizer_dir = model_dir\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_dir, trust_remote_code=True\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def build_prompt(history):\n",
    "    prompt = welcome_prompt\n",
    "    for query, response in history:\n",
    "        prompt += f\"\\n\\n用户：{query}\"\n",
    "        prompt += f\"\\n\\nChatGLM3-6B：{response}\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def main():\n",
    "    model_dir = 'output/checkpoint-2000' # 输入微调后的Checkpoint目录地址\n",
    "    model, tokenizer = load_model_and_tokenizer(model_dir)\n",
    "\n",
    "    past_key_values, history = None, []\n",
    "    global stop_stream\n",
    "    print(welcome_prompt)\n",
    "    while True:\n",
    "        query = input(\"\\n用户：\")\n",
    "        if query.strip().lower() == \"stop\":\n",
    "            break\n",
    "        if query.strip().lower() == \"clear\":\n",
    "            past_key_values, history = None, []\n",
    "            os.system(clear_command)\n",
    "            print(welcome_prompt)\n",
    "            continue\n",
    "\n",
    "        # Attempt to generate a response\n",
    "        try:\n",
    "            print(\"\\nChatGLM：\", end=\"\")\n",
    "            current_length = 0\n",
    "            response_generated = False\n",
    "            for response, history, past_key_values in model.stream_chat(\n",
    "                tokenizer, query, history=history, top_p=1,\n",
    "                temperature=0.01,\n",
    "                past_key_values=past_key_values,\n",
    "                return_past_key_values=True,\n",
    "                bad_words_ids=bad_word_ids  # assuming this is implemented correctly\n",
    "            ):\n",
    "                response_generated = True\n",
    "                # Check if the response contains any bad words\n",
    "                if any(bad_word in response for bad_word in bad_words):\n",
    "                    print(\"我的回答涉嫌了 bad word\")\n",
    "                    break  # Break the loop if a bad word is detected\n",
    "\n",
    "                # Otherwise, print the generated response\n",
    "                print(response[current_length:], end=\"\", flush=True)\n",
    "                current_length = len(response)\n",
    "            if not response_generated:\n",
    "                print(\"没有生成任何回答。\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"生成文本时发生错误：{e}，这可能是涉及到设定的敏感词汇\")\n",
    "\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf902e0-7bac-4db0-976c-775036ef8ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22154ef49c0f4433ace0ec2e0f2a46e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欢迎使用 ChatGLM3-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： 你好\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ChatGLM：生成文本时发生错误：probability tensor contains either `inf`, `nan` or element < 0，这可能是涉及到设定的敏感词汇\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： ChatGLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ChatGLM：生成文本时发生错误：probability tensor contains either `inf`, `nan` or element < 0，这可能是涉及到设定的敏感词汇\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： 类型#裙*裙长#半身裙\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ChatGLM：这款半身裙的版型设计非常合身，能够完美地展现出你的身材曲线，让你在穿着上更加有型有范。而裙身的设计更是别具一格，采用了一款精致的腰带，能够起到很好的收腰效果，让你穿着上更加有型。而裙身的设计更是别具一格，采用了一款精致的腰带，"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
