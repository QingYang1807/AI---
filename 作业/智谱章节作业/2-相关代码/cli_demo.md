# Êï¥ÂêàÂâçÊµãËØï


```python
ll /root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b
```

    total 12195720
    -rw------- 1 root       4133 Apr  3 01:14 MODEL_LICENSE
    -rw------- 1 root       4478 Apr  3 01:19 README.md
    -rw------- 1 root       1317 Apr  3 01:14 config.json
    -rw------- 1 root         37 Apr  3 01:14 configuration.json
    -rw------- 1 root       2332 Apr  3 01:14 configuration_chatglm.py
    -rw------- 1 root      55596 Apr  3 01:14 modeling_chatglm.py
    -rw------- 1 root 1827781090 Apr  3 01:15 pytorch_model-00001-of-00007.bin
    -rw------- 1 root 1968299480 Apr  3 01:16 pytorch_model-00002-of-00007.bin
    -rw------- 1 root 1927415036 Apr  3 01:17 pytorch_model-00003-of-00007.bin
    -rw------- 1 root 1815225998 Apr  3 01:17 pytorch_model-00004-of-00007.bin
    -rw------- 1 root 1968299544 Apr  3 01:18 pytorch_model-00005-of-00007.bin
    -rw------- 1 root 1927415036 Apr  3 01:19 pytorch_model-00006-of-00007.bin
    -rw------- 1 root 1052808542 Apr  3 01:19 pytorch_model-00007-of-00007.bin
    -rw------- 1 root      20437 Apr  3 01:19 pytorch_model.bin.index.json
    -rw------- 1 root      14692 Apr  3 01:19 quantization.py
    -rw------- 1 root      11279 Apr  3 01:19 tokenization_chatglm.py
    -rw------- 1 root    1018370 Apr  3 01:19 tokenizer.model
    -rw------- 1 root        244 Apr  3 01:19 tokenizer_config.json



```python
import os
import platform
from transformers import AutoTokenizer, AutoModel

MODEL_PATH = '/root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b'
TOKENIZER_PATH = '/root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b'

tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, trust_remote_code=True)
model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True, device_map="auto").eval()

os_name = platform.system()
clear_command = 'cls' if os_name == 'Windows' else 'clear'
stop_stream = False

welcome_prompt = "Ê¨¢Ëøé‰ΩøÁî® ChatGLM3-6B Ê®°ÂûãÔºåËæìÂÖ•ÂÜÖÂÆπÂç≥ÂèØËøõË°åÂØπËØùÔºåclear Ê∏ÖÁ©∫ÂØπËØùÂéÜÂè≤Ôºåstop ÁªàÊ≠¢Á®ãÂ∫è"
```


    Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]


    /root/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
      return self.fget.__get__(instance, owner)()



```python
def build_prompt(history):
    prompt = welcome_prompt
    for query, response in history:
        prompt += f"\n\nÁî®Êà∑Ôºö{query}"
        prompt += f"\n\nChatGLM3-6BÔºö{response}"
    return prompt
```


```python
past_key_values, history = None, []
global stop_stream
print(welcome_prompt)
while True:
    query = input("\nÁî®Êà∑Ôºö")
    if query.strip() == "stop":
        break
    if query.strip() == "clear":
        past_key_values, history = None, []
        os.system(clear_command)
        print(welcome_prompt)
        continue
    print("\nChatGLMÔºö", end="")
    current_length = 0
    for response, history, past_key_values in model.stream_chat(tokenizer, query, history=history, top_p=1,
                                                                temperature=0.01,
                                                                past_key_values=past_key_values,
                                                                return_past_key_values=True):
        if stop_stream:
            stop_stream = False
            break
        else:
            print(response[current_length:], end="", flush=True)
            current_length = len(response)
    print("")
```

    Ê¨¢Ëøé‰ΩøÁî® ChatGLM3-6B Ê®°ÂûãÔºåËæìÂÖ•ÂÜÖÂÆπÂç≥ÂèØËøõË°åÂØπËØùÔºåclear Ê∏ÖÁ©∫ÂØπËØùÂéÜÂè≤Ôºåstop ÁªàÊ≠¢Á®ãÂ∫è


    
    Áî®Êà∑Ôºö ‰Ω†Â•Ω


    
    ChatGLMÔºö‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM3-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ


    
    Áî®Êà∑Ôºö Á±ªÂûã#Ë£ô*Ë£ôÈïø#ÂçäË∫´Ë£ô


    
    ChatGLMÔºö‰Ω†Â•ΩÔºÅÊ†πÊçÆ‰Ω†ÁöÑÊèèËø∞ÔºåÊàëÁêÜËß£‰Ω†ÊÉ≥‰∫ÜËß£ÊúâÂÖ≥‚ÄúË£ô‚ÄùËøô‰∏™Á±ªÂûãÁöÑ‰ø°ÊÅØÔºåÁâπÂà´ÊòØ‚ÄúË£ôÈïø‚Äù‰∏∫‚ÄúÂçäË∫´Ë£ô‚ÄùÁöÑÁõ∏ÂÖ≥ÂÜÖÂÆπ„ÄÇ‰∏ãÈù¢ÊòØÊàë‰∏∫‰Ω†ÂáÜÂ§áÁöÑ‰∏Ä‰∫õ‰ø°ÊÅØÔºö
    
    ÂçäË∫´Ë£ôÊòØ‰∏ÄÁßçÈïøÂ∫¶ÂèäËÜù‰ª•‰∏ãÁöÑË£ôÂ≠êÔºåÈÄöÂ∏∏Âà∞ËÜùÁõñÈôÑËøë„ÄÇÂÆÉÊòØ‰∏ÄÁßçÈùûÂ∏∏ÂèóÊ¨¢ËøéÁöÑË£ôÂ≠êÁ±ªÂûãÔºåÂèØ‰ª•Êê≠ÈÖçÂ§öÁßç‰∏äË°£ÔºåÂπ∂ÈÄÇÁî®‰∫éÂêÑÁßçÂú∫Âêà„ÄÇÂçäË∫´Ë£ôÂèØ‰ª•Áî±ÂêÑÁßçÊùêÊñôÂà∂ÊàêÔºåÂ¶ÇÊ£â„ÄÅ‰∏ùÁª∏„ÄÅÁâõ‰ªîÂ∏ÉÁ≠â„ÄÇ
    
    ÂçäË∫´Ë£ôÁöÑÊ¨æÂºèÂ§öÁßçÂ§öÊ†∑ÔºåÊúâÂåÖËáÄÂûã„ÄÅAÂ≠óÂûã„ÄÅÁôæË§∂ÂûãÁ≠â„ÄÇÂÖ∂‰∏≠ÔºåÂåÖËáÄÂûãÂçäË∫´Ë£ôÈÄÇÂêàÊõ≤Á∫øÁé≤ÁèëÁöÑË∫´ÊùêÔºåAÂ≠óÂûãÂçäË∫´Ë£ôÂàôÈÄÇÂêàÁü©ÂΩ¢Ë∫´ÊùêÔºåËÄåÁôæË§∂ÂûãÂçäË∫´Ë£ôÂàôÈÄÇÂêà‰∏∞Êª°ÁöÑË∫´Êùê„ÄÇ
    
    ÂçäË∫´Ë£ôÂèØ‰ª•Êê≠ÈÖçÂ§öÁßç‰∏äË°£ÔºåÂ¶ÇTÊÅ§„ÄÅË°¨Ë°´„ÄÅÊØõË°£Á≠â„ÄÇÂú®Êê≠ÈÖçÊó∂ÔºåÂèØ‰ª•Ê†πÊçÆÂú∫Âêà„ÄÅ‰∏™‰∫∫ÂñúÂ•ΩÂíåË∫´ÊùêÁâπÁÇπËøõË°åÈÄâÊã©„ÄÇ‰æãÂ¶ÇÔºåÂú®Â§èÂ≠£ÔºåÂèØ‰ª•Êê≠ÈÖçËΩªËñÑËàíÈÄÇÁöÑTÊÅ§ÂíåÂáâÈûãÔºõÂú®ÂÜ¨Â≠£ÔºåÂèØ‰ª•Êê≠ÈÖçÂéöÂÆûÁöÑÊØõË°£ÂíåÈù¥Â≠ê„ÄÇ
    
    ÊÄª‰πãÔºåÂçäË∫´Ë£ôÊòØ‰∏ÄÁßçÈùûÂ∏∏ÂèóÊ¨¢ËøéÁöÑË£ôÂ≠êÁ±ªÂûãÔºåÈÄÇÂêàÂêÑÁßçÂú∫ÂêàÂíåË∫´ÊùêÁâπÁÇπ„ÄÇÂ∏åÊúõÊàëÁöÑÂõûÁ≠îËÉΩÂØπ‰Ω†ÊúâÊâÄÂ∏ÆÂä©ÔºÅ



    ---------------------------------------------------------------------------

    KeyboardInterrupt                         Traceback (most recent call last)

    Cell In[6], line 5
          3 print(welcome_prompt)
          4 while True:
    ----> 5     query = input("\nÁî®Êà∑Ôºö")
          6     if query.strip() == "stop":
          7         break


    File ~/miniconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py:1262, in Kernel.raw_input(self, prompt)
       1260     msg = "raw_input was called, but this frontend does not support input requests."
       1261     raise StdinNotImplementedError(msg)
    -> 1262 return self._input_request(
       1263     str(prompt),
       1264     self._parent_ident["shell"],
       1265     self.get_parent("shell"),
       1266     password=False,
       1267 )


    File ~/miniconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py:1305, in Kernel._input_request(self, prompt, ident, parent, password)
       1302 except KeyboardInterrupt:
       1303     # re-raise KeyboardInterrupt, to truncate traceback
       1304     msg = "Interrupted by user"
    -> 1305     raise KeyboardInterrupt(msg) from None
       1306 except Exception:
       1307     self.log.warning("Invalid Message:", exc_info=True)


    KeyboardInterrupt: Interrupted by user



```python
def main():
    past_key_values, history = None, []
    global stop_stream
    print(welcome_prompt)
    while True:
        query = input("\nÁî®Êà∑Ôºö")
        if query.strip() == "stop":
            break
        if query.strip() == "clear":
            past_key_values, history = None, []
            os.system(clear_command)
            print(welcome_prompt)
            continue
        print("\nChatGLMÔºö", end="")
        current_length = 0
        for response, history, past_key_values in model.stream_chat(tokenizer, query, history=history, top_p=1,
                                                                    temperature=0.01,
                                                                    past_key_values=past_key_values,
                                                                    return_past_key_values=True):
            if stop_stream:
                stop_stream = False
                break
            else:
                print(response[current_length:], end="", flush=True)
                current_length = len(response)
        print("")

```


```python
# ÂÆö‰πâÊ®°ÂûãÂíåÂàÜËØçÂô®Âä†ËΩΩÂáΩÊï∞
# Ëøô‰∏™ÂáΩÊï∞Ë¥üË¥£‰ªéÁªôÂÆöÁöÑÁõÆÂΩïÂä†ËΩΩÊ®°ÂûãÂíåÂàÜËØçÂô®„ÄÇÂÆÉÈ¶ñÂÖàÂ∞ÜËæìÂÖ•Ë∑ØÂæÑÊ†áÂáÜÂåñÔºåÁÑ∂ÂêéÊ£ÄÊü•ÊòØÂê¶Â≠òÂú®adapter_config.jsonÊñá‰ª∂Êù•ÂÜ≥ÂÆöÊòØÂä†ËΩΩÊ†áÂáÜÁöÑAutoModelForCausalLMÊ®°ÂûãËøòÊòØÁâπÊÆäÁöÑAutoPeftModelForCausalLMÊ®°Âûã„ÄÇÊ†πÊçÆÊâÄÂä†ËΩΩÁöÑÊ®°ÂûãÁ±ªÂûãÔºåÂÆÉËøòÂÜ≥ÂÆö‰ªéÂì™‰∏™ÁõÆÂΩïÂä†ËΩΩÂàÜËØçÂô®„ÄÇ
from pathlib import Path
from typing import Annotated, Union
from peft import AutoPeftModelForCausalLM, PeftModelForCausalLM
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    PreTrainedModel,
    PreTrainedTokenizer,
    PreTrainedTokenizerFast,
)

ModelType = Union[PreTrainedModel, PeftModelForCausalLM]
TokenizerType = Union[PreTrainedTokenizer, PreTrainedTokenizerFast]


def load_model_and_tokenizer(model_dir: Union[str, Path]) -> tuple[ModelType, TokenizerType]:
    model_dir = _resolve_path(model_dir)
    if (model_dir / 'adapter_config.json').exists():
        model = AutoPeftModelForCausalLM.from_pretrained(
            model_dir, trust_remote_code=True, device_map='auto'
        )
        tokenizer_dir = model.peft_config['default'].base_model_name_or_path
    else:
        model = AutoModelForCausalLM.from_pretrained(
            model_dir, trust_remote_code=True, device_map='auto'
        )
        tokenizer_dir = model_dir
    tokenizer = AutoTokenizer.from_pretrained(
        tokenizer_dir, trust_remote_code=True
    )
    return model, tokenizer

```


```python
def _resolve_path(path: Union[str, Path]) -> Path:
    return Path(path).expanduser().resolve()
```


```python
model_dir = 'output/checkpoint-2000'
```


```python
 model, tokenizer = load_model_and_tokenizer(model_dir)
```


    Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]



```python
prompt = 'Á±ªÂûã#Ë£ô*Ë£ôÈïø#ÂçäË∫´Ë£ô'
response, _ = model.chat(tokenizer, prompt)
# ÊúÄÂêéÔºåÊâìÂç∞Âá∫ÂìçÂ∫î„ÄÇ
print(response)
```

    ËøôÊ¨æÂçäË∫´Ë£ôÈááÁî®ÊüîËΩØÁöÑÁ∫ØÊ£âÈù¢ÊñôÔºå‰∫≤ËÇ§ËàíÈÄÇÔºå‰∏çÂà∫ÊøÄÁöÆËÇ§ÔºåÁ©øÁùÄËàíÈÄÇ„ÄÇÊó∂Â∞öÁöÑÁâàÂûãËÆæËÆ°ÔºåÁ©øËµ∑Êù•ÂæàÊòæÁò¶„ÄÇÊó∂Â∞öÁöÑËç∑Âè∂ËæπË£ôÊëÜÔºåÂá∏ÊòæÂá∫Â•≥ÊÄßÊüîÁæéÁöÑÊ∞îË¥®„ÄÇ


# Êï¥ÂêàÂêéÁöÑcli_demo.py


```python
import os
import platform
from pathlib import Path
from typing import Annotated, Union
from peft import AutoPeftModelForCausalLM, PeftModelForCausalLM
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    PreTrainedModel,
    PreTrainedTokenizer,
    PreTrainedTokenizerFast,
    AutoModel,
)

model_dir = 'output/checkpoint-2000' # ËæìÂÖ•ÂæÆË∞ÉÂêéÁöÑCheckpointÁõÆÂΩïÂú∞ÂùÄ
model, tokenizer = load_model_and_tokenizer(model_dir)


ModelType = Union[PreTrainedModel, PeftModelForCausalLM]
TokenizerType = Union[PreTrainedTokenizer, PreTrainedTokenizerFast]


def load_model_and_tokenizer(model_dir: Union[str, Path]) -> tuple[ModelType, TokenizerType]:
    model_dir = _resolve_path(model_dir)
    if (model_dir / 'adapter_config.json').exists():
        model = AutoPeftModelForCausalLM.from_pretrained(
            model_dir, trust_remote_code=True, device_map='auto'
        )
        tokenizer_dir = model.peft_config['default'].base_model_name_or_path
    else:
        model = AutoModelForCausalLM.from_pretrained(
            model_dir, trust_remote_code=True, device_map='auto'
        )
        tokenizer_dir = model_dir
    tokenizer = AutoTokenizer.from_pretrained(
        tokenizer_dir, trust_remote_code=True
    )
    return model, tokenizer

def main():
    past_key_values, history = None, []
    global stop_stream
    print(welcome_prompt)
    while True:
        query = input("\nÁî®Êà∑Ôºö")
        if query.strip() == "stop":
            break
        if query.strip() == "clear":
            past_key_values, history = None, []
            os.system(clear_command)
            print(welcome_prompt)
            continue
        print("\nChatGLMÔºö", end="")
        current_length = 0
        for response, history, past_key_values in model.stream_chat(tokenizer, query, history=history, top_p=1,
                                                                    temperature=0.01,
                                                                    past_key_values=past_key_values,
                                                                    return_past_key_values=True):
            if stop_stream:
                stop_stream = False
                break
            else:
                print(response[current_length:], end="", flush=True)
                current_length = len(response)
        print("")

```


    Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]



```python
main()
```

    Ê¨¢Ëøé‰ΩøÁî® ChatGLM3-6B Ê®°ÂûãÔºåËæìÂÖ•ÂÜÖÂÆπÂç≥ÂèØËøõË°åÂØπËØùÔºåclear Ê∏ÖÁ©∫ÂØπËØùÂéÜÂè≤Ôºåstop ÁªàÊ≠¢Á®ãÂ∫è


    
    Áî®Êà∑Ôºö Á±ªÂûã#Ë£ô*Ë£ôÈïø#ÂçäË∫´Ë£ô


    
    ChatGLMÔºöËøôÊ¨æÂçäË∫´Ë£ôÈááÁî®‰ºòË¥®Èù¢ÊñôÔºåÊüîËΩØËàíÈÄÇÔºåÁ©øÁùÄËàíÈÄÇÔºå‰∏çÈÄèËßÜÔºå‰∏çÈÄèÈ£éÔºåÁ©øÁùÄËàíÈÄÇÔºå‰∏çÂãíËÇâ„ÄÇË£ôË∫´ÈááÁî®Á∫ØËâ≤ËÆæËÆ°ÔºåÁÆÄÁ∫¶Â§ßÊñπÔºåÁôæÊê≠ÂÆûÁ©ø„ÄÇË£ôÊëÜÈááÁî®‰∏çËßÑÂàôËÆæËÆ°ÔºåÊó∂Â∞öÊúâÂûãÔºåÊê≠ÈÖçËµ∑Êù•Êõ¥ÊúâÂ±ÇÊ¨°ÊÑü„ÄÇ


    
    Áî®Êà∑Ôºö stop

