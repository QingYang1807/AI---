{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4e478b-c7d5-480b-8b6d-f78ee3b5114e",
   "metadata": {},
   "source": [
    "# Êï¥ÂêàÂâçÊµãËØï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9190e3c0-5a38-4f81-a3ff-8571d71a7a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12195720\n",
      "-rw------- 1 root       4133 Apr  3 01:14 MODEL_LICENSE\n",
      "-rw------- 1 root       4478 Apr  3 01:19 README.md\n",
      "-rw------- 1 root       1317 Apr  3 01:14 config.json\n",
      "-rw------- 1 root         37 Apr  3 01:14 configuration.json\n",
      "-rw------- 1 root       2332 Apr  3 01:14 configuration_chatglm.py\n",
      "-rw------- 1 root      55596 Apr  3 01:14 modeling_chatglm.py\n",
      "-rw------- 1 root 1827781090 Apr  3 01:15 pytorch_model-00001-of-00007.bin\n",
      "-rw------- 1 root 1968299480 Apr  3 01:16 pytorch_model-00002-of-00007.bin\n",
      "-rw------- 1 root 1927415036 Apr  3 01:17 pytorch_model-00003-of-00007.bin\n",
      "-rw------- 1 root 1815225998 Apr  3 01:17 pytorch_model-00004-of-00007.bin\n",
      "-rw------- 1 root 1968299544 Apr  3 01:18 pytorch_model-00005-of-00007.bin\n",
      "-rw------- 1 root 1927415036 Apr  3 01:19 pytorch_model-00006-of-00007.bin\n",
      "-rw------- 1 root 1052808542 Apr  3 01:19 pytorch_model-00007-of-00007.bin\n",
      "-rw------- 1 root      20437 Apr  3 01:19 pytorch_model.bin.index.json\n",
      "-rw------- 1 root      14692 Apr  3 01:19 quantization.py\n",
      "-rw------- 1 root      11279 Apr  3 01:19 tokenization_chatglm.py\n",
      "-rw------- 1 root    1018370 Apr  3 01:19 tokenizer.model\n",
      "-rw------- 1 root        244 Apr  3 01:19 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "ll /root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e670cc-5a37-4885-97cb-7f567db3d568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ea4953ae1e4566bfa90dd8632599fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "MODEL_PATH = '/root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b'\n",
    "TOKENIZER_PATH = '/root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True, device_map=\"auto\").eval()\n",
    "\n",
    "os_name = platform.system()\n",
    "clear_command = 'cls' if os_name == 'Windows' else 'clear'\n",
    "stop_stream = False\n",
    "\n",
    "welcome_prompt = \"Ê¨¢Ëøé‰ΩøÁî® ChatGLM3-6B Ê®°ÂûãÔºåËæìÂÖ•ÂÜÖÂÆπÂç≥ÂèØËøõË°åÂØπËØùÔºåclear Ê∏ÖÁ©∫ÂØπËØùÂéÜÂè≤Ôºåstop ÁªàÊ≠¢Á®ãÂ∫è\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "882b8bfd-f556-4a6d-a2a3-98767d9d73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(history):\n",
    "    prompt = welcome_prompt\n",
    "    for query, response in history:\n",
    "        prompt += f\"\\n\\nÁî®Êà∑Ôºö{query}\"\n",
    "        prompt += f\"\\n\\nChatGLM3-6BÔºö{response}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb5e049d-5497-49a6-97ad-768eff4de362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ê¨¢Ëøé‰ΩøÁî® ChatGLM3-6B Ê®°ÂûãÔºåËæìÂÖ•ÂÜÖÂÆπÂç≥ÂèØËøõË°åÂØπËØùÔºåclear Ê∏ÖÁ©∫ÂØπËØùÂéÜÂè≤Ôºåstop ÁªàÊ≠¢Á®ãÂ∫è\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Áî®Êà∑Ôºö ‰Ω†Â•Ω\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ChatGLMÔºö‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM3-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Áî®Êà∑Ôºö Á±ªÂûã#Ë£ô*Ë£ôÈïø#ÂçäË∫´Ë£ô\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ChatGLMÔºö‰Ω†Â•ΩÔºÅÊ†πÊçÆ‰Ω†ÁöÑÊèèËø∞ÔºåÊàëÁêÜËß£‰Ω†ÊÉ≥‰∫ÜËß£ÊúâÂÖ≥‚ÄúË£ô‚ÄùËøô‰∏™Á±ªÂûãÁöÑ‰ø°ÊÅØÔºåÁâπÂà´ÊòØ‚ÄúË£ôÈïø‚Äù‰∏∫‚ÄúÂçäË∫´Ë£ô‚ÄùÁöÑÁõ∏ÂÖ≥ÂÜÖÂÆπ„ÄÇ‰∏ãÈù¢ÊòØÊàë‰∏∫‰Ω†ÂáÜÂ§áÁöÑ‰∏Ä‰∫õ‰ø°ÊÅØÔºö\n",
      "\n",
      "ÂçäË∫´Ë£ôÊòØ‰∏ÄÁßçÈïøÂ∫¶ÂèäËÜù‰ª•‰∏ãÁöÑË£ôÂ≠êÔºåÈÄöÂ∏∏Âà∞ËÜùÁõñÈôÑËøë„ÄÇÂÆÉÊòØ‰∏ÄÁßçÈùûÂ∏∏ÂèóÊ¨¢ËøéÁöÑË£ôÂ≠êÁ±ªÂûãÔºåÂèØ‰ª•Êê≠ÈÖçÂ§öÁßç‰∏äË°£ÔºåÂπ∂ÈÄÇÁî®‰∫éÂêÑÁßçÂú∫Âêà„ÄÇÂçäË∫´Ë£ôÂèØ‰ª•Áî±ÂêÑÁßçÊùêÊñôÂà∂ÊàêÔºåÂ¶ÇÊ£â„ÄÅ‰∏ùÁª∏„ÄÅÁâõ‰ªîÂ∏ÉÁ≠â„ÄÇ\n",
      "\n",
      "ÂçäË∫´Ë£ôÁöÑÊ¨æÂºèÂ§öÁßçÂ§öÊ†∑ÔºåÊúâÂåÖËáÄÂûã„ÄÅAÂ≠óÂûã„ÄÅÁôæË§∂ÂûãÁ≠â„ÄÇÂÖ∂‰∏≠ÔºåÂåÖËáÄÂûãÂçäË∫´Ë£ôÈÄÇÂêàÊõ≤Á∫øÁé≤ÁèëÁöÑË∫´ÊùêÔºåAÂ≠óÂûãÂçäË∫´Ë£ôÂàôÈÄÇÂêàÁü©ÂΩ¢Ë∫´ÊùêÔºåËÄåÁôæË§∂ÂûãÂçäË∫´Ë£ôÂàôÈÄÇÂêà‰∏∞Êª°ÁöÑË∫´Êùê„ÄÇ\n",
      "\n",
      "ÂçäË∫´Ë£ôÂèØ‰ª•Êê≠ÈÖçÂ§öÁßç‰∏äË°£ÔºåÂ¶ÇTÊÅ§„ÄÅË°¨Ë°´„ÄÅÊØõË°£Á≠â„ÄÇÂú®Êê≠ÈÖçÊó∂ÔºåÂèØ‰ª•Ê†πÊçÆÂú∫Âêà„ÄÅ‰∏™‰∫∫ÂñúÂ•ΩÂíåË∫´ÊùêÁâπÁÇπËøõË°åÈÄâÊã©„ÄÇ‰æãÂ¶ÇÔºåÂú®Â§èÂ≠£ÔºåÂèØ‰ª•Êê≠ÈÖçËΩªËñÑËàíÈÄÇÁöÑTÊÅ§ÂíåÂáâÈûãÔºõÂú®ÂÜ¨Â≠£ÔºåÂèØ‰ª•Êê≠ÈÖçÂéöÂÆûÁöÑÊØõË°£ÂíåÈù¥Â≠ê„ÄÇ\n",
      "\n",
      "ÊÄª‰πãÔºåÂçäË∫´Ë£ôÊòØ‰∏ÄÁßçÈùûÂ∏∏ÂèóÊ¨¢ËøéÁöÑË£ôÂ≠êÁ±ªÂûãÔºåÈÄÇÂêàÂêÑÁßçÂú∫ÂêàÂíåË∫´ÊùêÁâπÁÇπ„ÄÇÂ∏åÊúõÊàëÁöÑÂõûÁ≠îËÉΩÂØπ‰Ω†ÊúâÊâÄÂ∏ÆÂä©ÔºÅ\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(welcome_prompt)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mÁî®Êà∑Ôºö\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "past_key_values, history = None, []\n",
    "global stop_stream\n",
    "print(welcome_prompt)\n",
    "while True:\n",
    "    query = input(\"\\nÁî®Êà∑Ôºö\")\n",
    "    if query.strip() == \"stop\":\n",
    "        break\n",
    "    if query.strip() == \"clear\":\n",
    "        past_key_values, history = None, []\n",
    "        os.system(clear_command)\n",
    "        print(welcome_prompt)\n",
    "        continue\n",
    "    print(\"\\nChatGLMÔºö\", end=\"\")\n",
    "    current_length = 0\n",
    "    for response, history, past_key_values in model.stream_chat(tokenizer, query, history=history, top_p=1,\n",
    "                                                                temperature=0.01,\n",
    "                                                                past_key_values=past_key_values,\n",
    "                                                                return_past_key_values=True):\n",
    "        if stop_stream:\n",
    "            stop_stream = False\n",
    "            break\n",
    "        else:\n",
    "            print(response[current_length:], end=\"\", flush=True)\n",
    "            current_length = len(response)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d60a11b0-8430-4ff9-b344-3eae2095737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    past_key_values, history = None, []\n",
    "    global stop_stream\n",
    "    print(welcome_prompt)\n",
    "    while True:\n",
    "        query = input(\"\\nÁî®Êà∑Ôºö\")\n",
    "        if query.strip() == \"stop\":\n",
    "            break\n",
    "        if query.strip() == \"clear\":\n",
    "            past_key_values, history = None, []\n",
    "            os.system(clear_command)\n",
    "            print(welcome_prompt)\n",
    "            continue\n",
    "        print(\"\\nChatGLMÔºö\", end=\"\")\n",
    "        current_length = 0\n",
    "        for response, history, past_key_values in model.stream_chat(tokenizer, query, history=history, top_p=1,\n",
    "                                                                    temperature=0.01,\n",
    "                                                                    past_key_values=past_key_values,\n",
    "                                                                    return_past_key_values=True):\n",
    "            if stop_stream:\n",
    "                stop_stream = False\n",
    "                break\n",
    "            else:\n",
    "                print(response[current_length:], end=\"\", flush=True)\n",
    "                current_length = len(response)\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e0ba834-988b-41ae-b5ad-b1fc0e3a81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂÆö‰πâÊ®°ÂûãÂíåÂàÜËØçÂô®Âä†ËΩΩÂáΩÊï∞\n",
    "# Ëøô‰∏™ÂáΩÊï∞Ë¥üË¥£‰ªéÁªôÂÆöÁöÑÁõÆÂΩïÂä†ËΩΩÊ®°ÂûãÂíåÂàÜËØçÂô®„ÄÇÂÆÉÈ¶ñÂÖàÂ∞ÜËæìÂÖ•Ë∑ØÂæÑÊ†áÂáÜÂåñÔºåÁÑ∂ÂêéÊ£ÄÊü•ÊòØÂê¶Â≠òÂú®adapter_config.jsonÊñá‰ª∂Êù•ÂÜ≥ÂÆöÊòØÂä†ËΩΩÊ†áÂáÜÁöÑAutoModelForCausalLMÊ®°ÂûãËøòÊòØÁâπÊÆäÁöÑAutoPeftModelForCausalLMÊ®°Âûã„ÄÇÊ†πÊçÆÊâÄÂä†ËΩΩÁöÑÊ®°ÂûãÁ±ªÂûãÔºåÂÆÉËøòÂÜ≥ÂÆö‰ªéÂì™‰∏™ÁõÆÂΩïÂä†ËΩΩÂàÜËØçÂô®„ÄÇ\n",
    "from pathlib import Path\n",
    "from typing import Annotated, Union\n",
    "from peft import AutoPeftModelForCausalLM, PeftModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerFast,\n",
    ")\n",
    "\n",
    "ModelType = Union[PreTrainedModel, PeftModelForCausalLM]\n",
    "TokenizerType = Union[PreTrainedTokenizer, PreTrainedTokenizerFast]\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(model_dir: Union[str, Path]) -> tuple[ModelType, TokenizerType]:\n",
    "    model_dir = _resolve_path(model_dir)\n",
    "    if (model_dir / 'adapter_config.json').exists():\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            model_dir, trust_remote_code=True, device_map='auto'\n",
    "        )\n",
    "        tokenizer_dir = model.peft_config['default'].base_model_name_or_path\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir, trust_remote_code=True, device_map='auto'\n",
    "        )\n",
    "        tokenizer_dir = model_dir\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_dir, trust_remote_code=True\n",
    "    )\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9566aa5a-1389-421b-960b-dc5a8d8f60c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94e6ce8b-710f-47e7-960e-c24c1d313c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'output/checkpoint-2000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a79fd8c-021e-491c-a097-fa7a1587b0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e08defa05944fd9377575e5c33cff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " model, tokenizer = load_model_and_tokenizer(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f56056a-179e-48cf-9207-298792722861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ËøôÊ¨æÂçäË∫´Ë£ôÈááÁî®ÊüîËΩØÁöÑÁ∫ØÊ£âÈù¢ÊñôÔºå‰∫≤ËÇ§ËàíÈÄÇÔºå‰∏çÂà∫ÊøÄÁöÆËÇ§ÔºåÁ©øÁùÄËàíÈÄÇ„ÄÇÊó∂Â∞öÁöÑÁâàÂûãËÆæËÆ°ÔºåÁ©øËµ∑Êù•ÂæàÊòæÁò¶„ÄÇÊó∂Â∞öÁöÑËç∑Âè∂ËæπË£ôÊëÜÔºåÂá∏ÊòæÂá∫Â•≥ÊÄßÊüîÁæéÁöÑÊ∞îË¥®„ÄÇ\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Á±ªÂûã#Ë£ô*Ë£ôÈïø#ÂçäË∫´Ë£ô'\n",
    "response, _ = model.chat(tokenizer, prompt)\n",
    "# ÊúÄÂêéÔºåÊâìÂç∞Âá∫ÂìçÂ∫î„ÄÇ\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06833e1d-c024-43ed-a310-24e8c254e42b",
   "metadata": {},
   "source": [
    "# Êï¥ÂêàÂêéÁöÑcli_demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e24e9f56-32f6-49fc-9fb4-d7ffd6819598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9446294cb1fd4bc0a63ed4ea9ace6584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "from pathlib import Path\n",
    "from typing import Annotated, Union\n",
    "from peft import AutoPeftModelForCausalLM, PeftModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerFast,\n",
    "    AutoModel,\n",
    ")\n",
    "\n",
    "model_dir = 'output/checkpoint-2000' # ËæìÂÖ•ÂæÆË∞ÉÂêéÁöÑCheckpointÁõÆÂΩïÂú∞ÂùÄ\n",
    "model, tokenizer = load_model_and_tokenizer(model_dir)\n",
    "\n",
    "\n",
    "ModelType = Union[PreTrainedModel, PeftModelForCausalLM]\n",
    "TokenizerType = Union[PreTrainedTokenizer, PreTrainedTokenizerFast]\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "    \n",
    "def load_model_and_tokenizer(model_dir: Union[str, Path]) -> tuple[ModelType, TokenizerType]:\n",
    "    model_dir = _resolve_path(model_dir)\n",
    "    if (model_dir / 'adapter_config.json').exists():\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            model_dir, trust_remote_code=True, device_map='auto'\n",
    "        )\n",
    "        tokenizer_dir = model.peft_config['default'].base_model_name_or_path\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir, trust_remote_code=True, device_map='auto'\n",
    "        )\n",
    "        tokenizer_dir = model_dir\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_dir, trust_remote_code=True\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "def main():\n",
    "    past_key_values, history = None, []\n",
    "    global stop_stream\n",
    "    print(welcome_prompt)\n",
    "    while True:\n",
    "        query = input(\"\\nÁî®Êà∑Ôºö\")\n",
    "        if query.strip() == \"stop\":\n",
    "            break\n",
    "        if query.strip() == \"clear\":\n",
    "            past_key_values, history = None, []\n",
    "            os.system(clear_command)\n",
    "            print(welcome_prompt)\n",
    "            continue\n",
    "        print(\"\\nChatGLMÔºö\", end=\"\")\n",
    "        current_length = 0\n",
    "        for response, history, past_key_values in model.stream_chat(tokenizer, query, history=history, top_p=1,\n",
    "                                                                    temperature=0.01,\n",
    "                                                                    past_key_values=past_key_values,\n",
    "                                                                    return_past_key_values=True):\n",
    "            if stop_stream:\n",
    "                stop_stream = False\n",
    "                break\n",
    "            else:\n",
    "                print(response[current_length:], end=\"\", flush=True)\n",
    "                current_length = len(response)\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "037c1d94-6f7e-4fc0-a989-19f48d6f0763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ê¨¢Ëøé‰ΩøÁî® ChatGLM3-6B Ê®°ÂûãÔºåËæìÂÖ•ÂÜÖÂÆπÂç≥ÂèØËøõË°åÂØπËØùÔºåclear Ê∏ÖÁ©∫ÂØπËØùÂéÜÂè≤Ôºåstop ÁªàÊ≠¢Á®ãÂ∫è\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Áî®Êà∑Ôºö Á±ªÂûã#Ë£ô*Ë£ôÈïø#ÂçäË∫´Ë£ô\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ChatGLMÔºöËøôÊ¨æÂçäË∫´Ë£ôÈááÁî®‰ºòË¥®Èù¢ÊñôÔºåÊüîËΩØËàíÈÄÇÔºåÁ©øÁùÄËàíÈÄÇÔºå‰∏çÈÄèËßÜÔºå‰∏çÈÄèÈ£éÔºåÁ©øÁùÄËàíÈÄÇÔºå‰∏çÂãíËÇâ„ÄÇË£ôË∫´ÈááÁî®Á∫ØËâ≤ËÆæËÆ°ÔºåÁÆÄÁ∫¶Â§ßÊñπÔºåÁôæÊê≠ÂÆûÁ©ø„ÄÇË£ôÊëÜÈááÁî®‰∏çËßÑÂàôËÆæËÆ°ÔºåÊó∂Â∞öÊúâÂûãÔºåÊê≠ÈÖçËµ∑Êù•Êõ¥ÊúâÂ±ÇÊ¨°ÊÑü„ÄÇ\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Áî®Êà∑Ôºö stop\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
