{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4e478b-c7d5-480b-8b6d-f78ee3b5114e",
   "metadata": {},
   "source": [
    "# æ•´åˆå‰æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9190e3c0-5a38-4f81-a3ff-8571d71a7a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12195720\n",
      "-rw------- 1 root       4133 Apr  3 01:14 MODEL_LICENSE\n",
      "-rw------- 1 root       4478 Apr  3 01:19 README.md\n",
      "-rw------- 1 root       1317 Apr  3 01:14 config.json\n",
      "-rw------- 1 root         37 Apr  3 01:14 configuration.json\n",
      "-rw------- 1 root       2332 Apr  3 01:14 configuration_chatglm.py\n",
      "-rw------- 1 root      55596 Apr  3 01:14 modeling_chatglm.py\n",
      "-rw------- 1 root 1827781090 Apr  3 01:15 pytorch_model-00001-of-00007.bin\n",
      "-rw------- 1 root 1968299480 Apr  3 01:16 pytorch_model-00002-of-00007.bin\n",
      "-rw------- 1 root 1927415036 Apr  3 01:17 pytorch_model-00003-of-00007.bin\n",
      "-rw------- 1 root 1815225998 Apr  3 01:17 pytorch_model-00004-of-00007.bin\n",
      "-rw------- 1 root 1968299544 Apr  3 01:18 pytorch_model-00005-of-00007.bin\n",
      "-rw------- 1 root 1927415036 Apr  3 01:19 pytorch_model-00006-of-00007.bin\n",
      "-rw------- 1 root 1052808542 Apr  3 01:19 pytorch_model-00007-of-00007.bin\n",
      "-rw------- 1 root      20437 Apr  3 01:19 pytorch_model.bin.index.json\n",
      "-rw------- 1 root      14692 Apr  3 01:19 quantization.py\n",
      "-rw------- 1 root      11279 Apr  3 01:19 tokenization_chatglm.py\n",
      "-rw------- 1 root    1018370 Apr  3 01:19 tokenizer.model\n",
      "-rw------- 1 root        244 Apr  3 01:19 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "ll /root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e670cc-5a37-4885-97cb-7f567db3d568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ea4953ae1e4566bfa90dd8632599fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "MODEL_PATH = '/root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b'\n",
    "TOKENIZER_PATH = '/root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True, device_map=\"auto\").eval()\n",
    "\n",
    "os_name = platform.system()\n",
    "clear_command = 'cls' if os_name == 'Windows' else 'clear'\n",
    "stop_stream = False\n",
    "\n",
    "welcome_prompt = \"æ¬¢è¿ä½¿ç”¨ ChatGLM3-6B æ¨¡å‹ï¼Œè¾“å…¥å†…å®¹å³å¯è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå¯¹è¯å†å²ï¼Œstop ç»ˆæ­¢ç¨‹åº\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "882b8bfd-f556-4a6d-a2a3-98767d9d73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(history):\n",
    "    prompt = welcome_prompt\n",
    "    for query, response in history:\n",
    "        prompt += f\"\\n\\nç”¨æˆ·ï¼š{query}\"\n",
    "        prompt += f\"\\n\\nChatGLM3-6Bï¼š{response}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb5e049d-5497-49a6-97ad-768eff4de362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¬¢è¿ä½¿ç”¨ ChatGLM3-6B æ¨¡å‹ï¼Œè¾“å…¥å†…å®¹å³å¯è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå¯¹è¯å†å²ï¼Œstop ç»ˆæ­¢ç¨‹åº\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ç”¨æˆ·ï¼š ä½ å¥½\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ChatGLMï¼šä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM3-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ç”¨æˆ·ï¼š ç±»å‹#è£™*è£™é•¿#åŠèº«è£™\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ChatGLMï¼šä½ å¥½ï¼æ ¹æ®ä½ çš„æè¿°ï¼Œæˆ‘ç†è§£ä½ æƒ³äº†è§£æœ‰å…³â€œè£™â€è¿™ä¸ªç±»å‹çš„ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯â€œè£™é•¿â€ä¸ºâ€œåŠèº«è£™â€çš„ç›¸å…³å†…å®¹ã€‚ä¸‹é¢æ˜¯æˆ‘ä¸ºä½ å‡†å¤‡çš„ä¸€äº›ä¿¡æ¯ï¼š\n",
      "\n",
      "åŠèº«è£™æ˜¯ä¸€ç§é•¿åº¦åŠè†ä»¥ä¸‹çš„è£™å­ï¼Œé€šå¸¸åˆ°è†ç›–é™„è¿‘ã€‚å®ƒæ˜¯ä¸€ç§éå¸¸å—æ¬¢è¿çš„è£™å­ç±»å‹ï¼Œå¯ä»¥æ­é…å¤šç§ä¸Šè¡£ï¼Œå¹¶é€‚ç”¨äºå„ç§åœºåˆã€‚åŠèº«è£™å¯ä»¥ç”±å„ç§ææ–™åˆ¶æˆï¼Œå¦‚æ£‰ã€ä¸ç»¸ã€ç‰›ä»”å¸ƒç­‰ã€‚\n",
      "\n",
      "åŠèº«è£™çš„æ¬¾å¼å¤šç§å¤šæ ·ï¼Œæœ‰åŒ…è‡€å‹ã€Aå­—å‹ã€ç™¾è¤¶å‹ç­‰ã€‚å…¶ä¸­ï¼ŒåŒ…è‡€å‹åŠèº«è£™é€‚åˆæ›²çº¿ç²ç‘çš„èº«æï¼ŒAå­—å‹åŠèº«è£™åˆ™é€‚åˆçŸ©å½¢èº«æï¼Œè€Œç™¾è¤¶å‹åŠèº«è£™åˆ™é€‚åˆä¸°æ»¡çš„èº«æã€‚\n",
      "\n",
      "åŠèº«è£™å¯ä»¥æ­é…å¤šç§ä¸Šè¡£ï¼Œå¦‚Tæ¤ã€è¡¬è¡«ã€æ¯›è¡£ç­‰ã€‚åœ¨æ­é…æ—¶ï¼Œå¯ä»¥æ ¹æ®åœºåˆã€ä¸ªäººå–œå¥½å’Œèº«æç‰¹ç‚¹è¿›è¡Œé€‰æ‹©ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤å­£ï¼Œå¯ä»¥æ­é…è½»è–„èˆ’é€‚çš„Tæ¤å’Œå‡‰é‹ï¼›åœ¨å†¬å­£ï¼Œå¯ä»¥æ­é…åšå®çš„æ¯›è¡£å’Œé´å­ã€‚\n",
      "\n",
      "æ€»ä¹‹ï¼ŒåŠèº«è£™æ˜¯ä¸€ç§éå¸¸å—æ¬¢è¿çš„è£™å­ç±»å‹ï¼Œé€‚åˆå„ç§åœºåˆå’Œèº«æç‰¹ç‚¹ã€‚å¸Œæœ›æˆ‘çš„å›ç­”èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(welcome_prompt)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mç”¨æˆ·ï¼š\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "past_key_values, history = None, []\n",
    "global stop_stream\n",
    "print(welcome_prompt)\n",
    "while True:\n",
    "    query = input(\"\\nç”¨æˆ·ï¼š\")\n",
    "    if query.strip() == \"stop\":\n",
    "        break\n",
    "    if query.strip() == \"clear\":\n",
    "        past_key_values, history = None, []\n",
    "        os.system(clear_command)\n",
    "        print(welcome_prompt)\n",
    "        continue\n",
    "    print(\"\\nChatGLMï¼š\", end=\"\")\n",
    "    current_length = 0\n",
    "    for response, history, past_key_values in model.stream_chat(tokenizer, query, history=history, top_p=1,\n",
    "                                                                temperature=0.01,\n",
    "                                                                past_key_values=past_key_values,\n",
    "                                                                return_past_key_values=True):\n",
    "        if stop_stream:\n",
    "            stop_stream = False\n",
    "            break\n",
    "        else:\n",
    "            print(response[current_length:], end=\"\", flush=True)\n",
    "            current_length = len(response)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d60a11b0-8430-4ff9-b344-3eae2095737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    past_key_values, history = None, []\n",
    "    global stop_stream\n",
    "    print(welcome_prompt)\n",
    "    while True:\n",
    "        query = input(\"\\nç”¨æˆ·ï¼š\")\n",
    "        if query.strip() == \"stop\":\n",
    "            break\n",
    "        if query.strip() == \"clear\":\n",
    "            past_key_values, history = None, []\n",
    "            os.system(clear_command)\n",
    "            print(welcome_prompt)\n",
    "            continue\n",
    "        print(\"\\nChatGLMï¼š\", end=\"\")\n",
    "        current_length = 0\n",
    "        for response, history, past_key_values in model.stream_chat(tokenizer, query, history=history, top_p=1,\n",
    "                                                                    temperature=0.01,\n",
    "                                                                    past_key_values=past_key_values,\n",
    "                                                                    return_past_key_values=True):\n",
    "            if stop_stream:\n",
    "                stop_stream = False\n",
    "                break\n",
    "            else:\n",
    "                print(response[current_length:], end=\"\", flush=True)\n",
    "                current_length = len(response)\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e0ba834-988b-41ae-b5ad-b1fc0e3a81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½å‡½æ•°\n",
    "# è¿™ä¸ªå‡½æ•°è´Ÿè´£ä»ç»™å®šçš„ç›®å½•åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚å®ƒé¦–å…ˆå°†è¾“å…¥è·¯å¾„æ ‡å‡†åŒ–ï¼Œç„¶åæ£€æŸ¥æ˜¯å¦å­˜åœ¨adapter_config.jsonæ–‡ä»¶æ¥å†³å®šæ˜¯åŠ è½½æ ‡å‡†çš„AutoModelForCausalLMæ¨¡å‹è¿˜æ˜¯ç‰¹æ®Šçš„AutoPeftModelForCausalLMæ¨¡å‹ã€‚æ ¹æ®æ‰€åŠ è½½çš„æ¨¡å‹ç±»å‹ï¼Œå®ƒè¿˜å†³å®šä»å“ªä¸ªç›®å½•åŠ è½½åˆ†è¯å™¨ã€‚\n",
    "from pathlib import Path\n",
    "from typing import Annotated, Union\n",
    "from peft import AutoPeftModelForCausalLM, PeftModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerFast,\n",
    ")\n",
    "\n",
    "ModelType = Union[PreTrainedModel, PeftModelForCausalLM]\n",
    "TokenizerType = Union[PreTrainedTokenizer, PreTrainedTokenizerFast]\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(model_dir: Union[str, Path]) -> tuple[ModelType, TokenizerType]:\n",
    "    model_dir = _resolve_path(model_dir)\n",
    "    if (model_dir / 'adapter_config.json').exists():\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            model_dir, trust_remote_code=True, device_map='auto'\n",
    "        )\n",
    "        tokenizer_dir = model.peft_config['default'].base_model_name_or_path\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir, trust_remote_code=True, device_map='auto'\n",
    "        )\n",
    "        tokenizer_dir = model_dir\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_dir, trust_remote_code=True\n",
    "    )\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9566aa5a-1389-421b-960b-dc5a8d8f60c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94e6ce8b-710f-47e7-960e-c24c1d313c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'output/checkpoint-2000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a79fd8c-021e-491c-a097-fa7a1587b0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e08defa05944fd9377575e5c33cff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " model, tokenizer = load_model_and_tokenizer(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f56056a-179e-48cf-9207-298792722861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¿™æ¬¾åŠèº«è£™é‡‡ç”¨æŸ”è½¯çš„çº¯æ£‰é¢æ–™ï¼Œäº²è‚¤èˆ’é€‚ï¼Œä¸åˆºæ¿€çš®è‚¤ï¼Œç©¿ç€èˆ’é€‚ã€‚æ—¶å°šçš„ç‰ˆå‹è®¾è®¡ï¼Œç©¿èµ·æ¥å¾ˆæ˜¾ç˜¦ã€‚æ—¶å°šçš„è·å¶è¾¹è£™æ‘†ï¼Œå‡¸æ˜¾å‡ºå¥³æ€§æŸ”ç¾çš„æ°”è´¨ã€‚\n"
     ]
    }
   ],
   "source": [
    "prompt = 'ç±»å‹#è£™*è£™é•¿#åŠèº«è£™'\n",
    "response, _ = model.chat(tokenizer, prompt)\n",
    "# æœ€åï¼Œæ‰“å°å‡ºå“åº”ã€‚\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06833e1d-c024-43ed-a310-24e8c254e42b",
   "metadata": {},
   "source": [
    "# æ•´åˆåçš„cli_demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e24e9f56-32f6-49fc-9fb4-d7ffd6819598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9446294cb1fd4bc0a63ed4ea9ace6584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "from pathlib import Path\n",
    "from typing import Annotated, Union\n",
    "from peft import AutoPeftModelForCausalLM, PeftModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerFast,\n",
    "    AutoModel,\n",
    ")\n",
    "\n",
    "model_dir = 'output/checkpoint-2000' # è¾“å…¥å¾®è°ƒåçš„Checkpointç›®å½•åœ°å€\n",
    "model, tokenizer = load_model_and_tokenizer(model_dir)\n",
    "\n",
    "\n",
    "ModelType = Union[PreTrainedModel, PeftModelForCausalLM]\n",
    "TokenizerType = Union[PreTrainedTokenizer, PreTrainedTokenizerFast]\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "    \n",
    "def load_model_and_tokenizer(model_dir: Union[str, Path]) -> tuple[ModelType, TokenizerType]:\n",
    "    model_dir = _resolve_path(model_dir)\n",
    "    if (model_dir / 'adapter_config.json').exists():\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            model_dir, trust_remote_code=True, device_map='auto'\n",
    "        )\n",
    "        tokenizer_dir = model.peft_config['default'].base_model_name_or_path\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir, trust_remote_code=True, device_map='auto'\n",
    "        )\n",
    "        tokenizer_dir = model_dir\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_dir, trust_remote_code=True\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "def main():\n",
    "    past_key_values, history = None, []\n",
    "    global stop_stream\n",
    "    print(welcome_prompt)\n",
    "    while True:\n",
    "        query = input(\"\\nç”¨æˆ·ï¼š\")\n",
    "        if query.strip() == \"stop\":\n",
    "            break\n",
    "        if query.strip() == \"clear\":\n",
    "            past_key_values, history = None, []\n",
    "            os.system(clear_command)\n",
    "            print(welcome_prompt)\n",
    "            continue\n",
    "        print(\"\\nChatGLMï¼š\", end=\"\")\n",
    "        current_length = 0\n",
    "        for response, history, past_key_values in model.stream_chat(tokenizer, query, history=history, top_p=1,\n",
    "                                                                    temperature=0.01,\n",
    "                                                                    past_key_values=past_key_values,\n",
    "                                                                    return_past_key_values=True):\n",
    "            if stop_stream:\n",
    "                stop_stream = False\n",
    "                break\n",
    "            else:\n",
    "                print(response[current_length:], end=\"\", flush=True)\n",
    "                current_length = len(response)\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "037c1d94-6f7e-4fc0-a989-19f48d6f0763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¬¢è¿ä½¿ç”¨ ChatGLM3-6B æ¨¡å‹ï¼Œè¾“å…¥å†…å®¹å³å¯è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå¯¹è¯å†å²ï¼Œstop ç»ˆæ­¢ç¨‹åº\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ç”¨æˆ·ï¼š ç±»å‹#è£™*è£™é•¿#åŠèº«è£™\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ChatGLMï¼šè¿™æ¬¾åŠèº«è£™é‡‡ç”¨ä¼˜è´¨é¢æ–™ï¼ŒæŸ”è½¯èˆ’é€‚ï¼Œç©¿ç€èˆ’é€‚ï¼Œä¸é€è§†ï¼Œä¸é€é£ï¼Œç©¿ç€èˆ’é€‚ï¼Œä¸å‹’è‚‰ã€‚è£™èº«é‡‡ç”¨çº¯è‰²è®¾è®¡ï¼Œç®€çº¦å¤§æ–¹ï¼Œç™¾æ­å®ç©¿ã€‚è£™æ‘†é‡‡ç”¨ä¸è§„åˆ™è®¾è®¡ï¼Œæ—¶å°šæœ‰å‹ï¼Œæ­é…èµ·æ¥æ›´æœ‰å±‚æ¬¡æ„Ÿã€‚\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ç”¨æˆ·ï¼š stop\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
