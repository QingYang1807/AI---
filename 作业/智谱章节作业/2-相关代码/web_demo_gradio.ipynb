{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fffa11e8-e69b-4402-96bf-252c83805212",
   "metadata": {},
   "source": [
    "# web_demo_gradio.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7c4a1f-c03c-4f97-9838-30a8932d566a",
   "metadata": {},
   "source": [
    "# 整合前测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6403a5-db22-46ab-862b-9cd839590345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting gradio\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/a7/06/29af2174e7aa1f3890e46483902bbb10628a90ddc053b2b02fcd201da89e/gradio-4.25.0-py3-none-any.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting python-multipart>=0.0.9\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/3d/47/444768600d9e0ebc82f8e347775d24aef8f6348cf00e9fa0e81910814e6d/python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in ./miniconda3/lib/python3.10/site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in ./miniconda3/lib/python3.10/site-packages (from gradio) (10.2.0)\n",
      "Collecting ruff>=0.2.2\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/5a/8d/cc5d7078ffd2779a38a05bcff8ffaa24f524de51c46223a2dd2710a8c50e/ruff-0.3.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2<4.0 in ./miniconda3/lib/python3.10/site-packages (from gradio) (3.1.2)\n",
      "Collecting orjson~=3.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/fb/c9/d43cc61ebd197d6b50bcc2f6dad8d4102eaf1750e1d7e52d0b7fa8296f0f/orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in ./miniconda3/lib/python3.10/site-packages (from gradio) (2.2.1)\n",
      "Requirement already satisfied: pydantic>=2.0 in ./miniconda3/lib/python3.10/site-packages (from gradio) (2.6.4)\n",
      "Collecting tomlkit==0.12.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/68/4f/12207897848a653d03ebbf6775a29d949408ded5f99b2d87198bc5c93508/tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Collecting aiofiles<24.0,>=22.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/c5/19/5af6804c4cc0fed83f47bff6e413a98a36618e7d40185cd36e69737f3b0e/aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting gradio-client==0.15.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/bc/63/534dfb5a0a3ff3ea6aa26dd394cef0e389f05bc15443fcb877b6a0cf2767/gradio_client-0.15.0-py3-none-any.whl (313 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.4/313.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fastapi\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/c0/c1/2dc286475c8e2e455e431a1cf1cf29662c9f9290434161088ba039d77481/fastapi-0.110.1-py3-none-any.whl (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib~=3.0 in ./miniconda3/lib/python3.10/site-packages (from gradio) (3.8.2)\n",
      "Collecting semantic-version~=2.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/6a/23/8146aad7d88f4fcb3a6218f41a60f6c2d4e3a72de72da1825dc7c8f7877c/semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in ./miniconda3/lib/python3.10/site-packages (from gradio) (0.22.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in ./miniconda3/lib/python3.10/site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: packaging in ./miniconda3/lib/python3.10/site-packages (from gradio) (23.2)\n",
      "Collecting uvicorn>=0.14.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/73/f5/cbb16fcbe277c1e0b8b3ddd188f2df0e0947f545c49119b589643632d156/uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in ./miniconda3/lib/python3.10/site-packages (from gradio) (6.0.1)\n",
      "Collecting pydub\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/a6/53/d78dc063216e62fc55f6b2eebb447f6a4b0a59f55c8406376f76bf959b08/pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting importlib-resources<7.0,>=1.3\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/75/06/4df55e1b7b112d183f65db9503bff189e97179b256e1ea450a3c365241e0/importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: numpy~=1.0 in ./miniconda3/lib/python3.10/site-packages (from gradio) (1.26.3)\n",
      "Collecting ffmpy\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/1d/70/07914754979f5dd80bda947a0ffd181c08bfcb137b01c3c0cef45254d271/ffmpy-0.3.2.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: httpx>=0.24.1 in ./miniconda3/lib/python3.10/site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: typer[all]<1.0,>=0.9 in ./miniconda3/lib/python3.10/site-packages (from gradio) (0.12.0)\n",
      "Collecting altair<6.0,>=4.2.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/46/30/2118537233fa72c1d91a81f5908a7e843a6601ccc68b76838ebc4951505f/altair-5.3.0-py3-none-any.whl (857 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.8/857.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting websockets<12.0,>=10.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/58/0a/7570e15661a0a546c3a1152d95fe8c05480459bab36247f0acbf41f01a41/websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in ./miniconda3/lib/python3.10/site-packages (from gradio-client==0.15.0->gradio) (2023.12.2)\n",
      "Requirement already satisfied: toolz in ./miniconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in ./miniconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.20.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./miniconda3/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: sniffio in ./miniconda3/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.3.0)\n",
      "Requirement already satisfied: anyio in ./miniconda3/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (4.2.0)\n",
      "Requirement already satisfied: certifi in ./miniconda3/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2022.12.7)\n",
      "Requirement already satisfied: idna in ./miniconda3/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./miniconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: requests in ./miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.64.1)\n",
      "Requirement already satisfied: filelock in ./miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./miniconda3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./miniconda3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./miniconda3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.47.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./miniconda3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./miniconda3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./miniconda3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./miniconda3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./miniconda3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./miniconda3/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in ./miniconda3/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.16.3)\n",
      "\u001b[33mWARNING: typer 0.12.0 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: typer-slim[standard]==0.12.0 in ./miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (0.12.0)\n",
      "Requirement already satisfied: typer-cli==0.12.0 in ./miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (0.12.0)\n",
      "Requirement already satisfied: click>=8.0.0 in ./miniconda3/lib/python3.10/site-packages (from typer-slim[standard]==0.12.0->typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./miniconda3/lib/python3.10/site-packages (from typer-slim[standard]==0.12.0->typer[all]<1.0,>=0.9->gradio) (13.7.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./miniconda3/lib/python3.10/site-packages (from typer-slim[standard]==0.12.0->typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
      "Collecting starlette<0.38.0,>=0.37.2\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/fd/18/31fa32ed6c68ba66220204ef0be798c349d0a20c1901f9d4a794e08c76d8/starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in ./miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.32.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.16.2)\n",
      "Requirement already satisfied: six>=1.5 in ./miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./miniconda3/lib/python3.10/site-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.4)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./miniconda3/lib/python3.10/site-packages (from rich>=10.11.0->typer-slim[standard]==0.12.0->typer[all]<1.0,>=0.9->gradio) (2.17.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./miniconda3/lib/python3.10/site-packages (from rich>=10.11.0->typer-slim[standard]==0.12.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./miniconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer-slim[standard]==0.12.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5583 sha256=68a8adb4ace2f8c79a4d6f82c4716dca2e6d31ab2ef085a3f39237fd2f978af4\n",
      "  Stored in directory: /root/.cache/pip/wheels/f1/33/b9/967f1c6df43a4e6759a80bcc23e335ad8ea0d3c7be950bdcc7\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: pydub, ffmpy, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, orjson, importlib-resources, aiofiles, starlette, gradio-client, fastapi, altair, gradio\n",
      "Successfully installed aiofiles-23.2.1 altair-5.3.0 fastapi-0.110.1 ffmpy-0.3.2 gradio-4.25.0 gradio-client-0.15.0 importlib-resources-6.4.0 orjson-3.10.0 pydub-0.25.1 python-multipart-0.0.9 ruff-0.3.5 semantic-version-2.10.0 starlette-0.37.2 tomlkit-0.12.0 uvicorn-0.29.0 websockets-11.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10744abd-77ed-476b-9a8f-df17f4fcec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import torch\n",
    "from threading import Thread\n",
    "\n",
    "from typing import Union, Annotated\n",
    "from pathlib import Path\n",
    "from peft import AutoPeftModelForCausalLM, PeftModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerFast,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    TextIteratorStreamer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeea7bdd-9539-4673-9009-3b952d4a377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelType = Union[PreTrainedModel, PeftModelForCausalLM]\n",
    "TokenizerType = Union[PreTrainedTokenizer, PreTrainedTokenizerFast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c96f0b11-b425-479e-bfb6-7dccb98695bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '/root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b'\n",
    "TOKENIZER_PATH = '/root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbfc9fde-78eb-4a7c-b303-8d0dc62facb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47ddd9c4-52c5-4cba-afc8-4bb244f32567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(\n",
    "        model_dir: Union[str, Path], trust_remote_code: bool = True\n",
    ") -> tuple[ModelType, TokenizerType]:\n",
    "    model_dir = _resolve_path(model_dir)\n",
    "    if (model_dir / 'adapter_config.json').exists():\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            model_dir, trust_remote_code=trust_remote_code, device_map='auto'\n",
    "        )\n",
    "        tokenizer_dir = model.peft_config['default'].base_model_name_or_path\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir, trust_remote_code=trust_remote_code, device_map='auto'\n",
    "        )\n",
    "        tokenizer_dir = model_dir\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_dir, trust_remote_code=trust_remote_code\n",
    "    )\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9069f6d8-ecb0-42ec-b75d-6ad695526992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c3debc44a94ebfa66fab5b2fa0031f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(MODEL_PATH, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33670707-bd4d-4650-baf0-a3f4cae92a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [0, 2]\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcf05b13-5307-4480-aef3-449835ea83fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if line != \"\"]\n",
    "    count = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"```\" in line:\n",
    "            count += 1\n",
    "            items = line.split('`')\n",
    "            if count % 2 == 1:\n",
    "                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n",
    "            else:\n",
    "                lines[i] = f'<br></code></pre>'\n",
    "        else:\n",
    "            if i > 0:\n",
    "                if count % 2 == 1:\n",
    "                    line = line.replace(\"`\", \"\\`\")\n",
    "                    line = line.replace(\"<\", \"&lt;\")\n",
    "                    line = line.replace(\">\", \"&gt;\")\n",
    "                    line = line.replace(\" \", \"&nbsp;\")\n",
    "                    line = line.replace(\"*\", \"&ast;\")\n",
    "                    line = line.replace(\"_\", \"&lowbar;\")\n",
    "                    line = line.replace(\"-\", \"&#45;\")\n",
    "                    line = line.replace(\".\", \"&#46;\")\n",
    "                    line = line.replace(\"!\", \"&#33;\")\n",
    "                    line = line.replace(\"(\", \"&#40;\")\n",
    "                    line = line.replace(\")\", \"&#41;\")\n",
    "                    line = line.replace(\"$\", \"&#36;\")\n",
    "                lines[i] = \"<br>\" + line\n",
    "    text = \"\".join(lines)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd40728c-9746-47bb-b078-6c002eae9151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(history, max_length, top_p, temperature):\n",
    "    stop = StopOnTokens()\n",
    "    messages = []\n",
    "    for idx, (user_msg, model_msg) in enumerate(history):\n",
    "        if idx == len(history) - 1 and not model_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            break\n",
    "        if user_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        if model_msg:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": model_msg})\n",
    "\n",
    "    print(\"\\n\\n====conversation====\\n\", messages)\n",
    "    model_inputs = tokenizer.apply_chat_template(messages,\n",
    "                                                 add_generation_prompt=True,\n",
    "                                                 tokenize=True,\n",
    "                                                 return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=60, skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = {\n",
    "        \"input_ids\": model_inputs,\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": max_length,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": top_p,\n",
    "        \"temperature\": temperature,\n",
    "        \"stopping_criteria\": StoppingCriteriaList([stop]),\n",
    "        \"repetition_penalty\": 1.2,\n",
    "    }\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    for new_token in streamer:\n",
    "        if new_token != '':\n",
    "            history[-1][1] += new_token\n",
    "            yield history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "255d788b-1561-487f-ad83-416311d670f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"\"\"<h1 align=\"center\">ChatGLM3-6B Gradio Simple Demo</h1>\"\"\")\n",
    "    chatbot = gr.Chatbot()\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            with gr.Column(scale=12):\n",
    "                user_input = gr.Textbox(show_label=False, placeholder=\"Input...\", lines=10, container=False)\n",
    "            with gr.Column(min_width=32, scale=1):\n",
    "                submitBtn = gr.Button(\"Submit\")\n",
    "        with gr.Column(scale=1):\n",
    "            emptyBtn = gr.Button(\"Clear History\")\n",
    "            max_length = gr.Slider(0, 32768, value=8192, step=1.0, label=\"Maximum length\", interactive=True)\n",
    "            top_p = gr.Slider(0, 1, value=0.8, step=0.01, label=\"Top P\", interactive=True)\n",
    "            temperature = gr.Slider(0.01, 1, value=0.6, step=0.01, label=\"Temperature\", interactive=True)\n",
    "\n",
    "\n",
    "    def user(query, history):\n",
    "        return \"\", history + [[parse_text(query), \"\"]]\n",
    "\n",
    "\n",
    "    submitBtn.click(user, [user_input, chatbot], [user_input, chatbot], queue=False).then(\n",
    "        predict, [chatbot, max_length, top_p, temperature], chatbot\n",
    "    )\n",
    "    emptyBtn.click(lambda: None, None, chatbot, queue=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "560f418b-9f44-43a3-96c1-3d63a650c74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gradio Blocks instance: 3 backend functions\n",
       "-------------------------------------------\n",
       "fn_index=0\n",
       " inputs:\n",
       " |-<gradio.components.textbox.Textbox object at 0x7fe6006160b0>\n",
       " |-<gradio.components.chatbot.Chatbot object at 0x7fe600658160>\n",
       " outputs:\n",
       " |-<gradio.components.textbox.Textbox object at 0x7fe6006160b0>\n",
       " |-<gradio.components.chatbot.Chatbot object at 0x7fe600658160>\n",
       "fn_index=1\n",
       " inputs:\n",
       " |-<gradio.components.chatbot.Chatbot object at 0x7fe600658160>\n",
       " |-<gradio.components.slider.Slider object at 0x7fe600617a90>\n",
       " |-<gradio.components.slider.Slider object at 0x7fe600616ec0>\n",
       " |-<gradio.components.slider.Slider object at 0x7fe600616ad0>\n",
       " outputs:\n",
       " |-<gradio.components.chatbot.Chatbot object at 0x7fe600658160>\n",
       "fn_index=2\n",
       " inputs:\n",
       " outputs:\n",
       " |-<gradio.components.chatbot.Chatbot object at 0x7fe600658160>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb4264a5-ad03-42fd-b6ed-fcdcd56100b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "Exception in thread Thread-12 (generate):\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1575, in generate\n",
      "    result = self._sample(\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2697, in _sample\n",
      "    outputs = self(\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/chatglm3-6b/modeling_chatglm.py\", line 937, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/chatglm3-6b/modeling_chatglm.py\", line 830, in forward\n",
      "    hidden_states, presents, all_hidden_states, all_self_attentions = self.encoder(\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/chatglm3-6b/modeling_chatglm.py\", line 640, in forward\n",
      "    layer_ret = layer(\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/chatglm3-6b/modeling_chatglm.py\", line 544, in forward\n",
      "    attention_output, kv_cache = self.self_attention(\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/chatglm3-6b/modeling_chatglm.py\", line 376, in forward\n",
      "    mixed_x_layer = self.query_key_value(hidden_states)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "RuntimeError: \"addmm_impl_cpu_\" not implemented for 'Half'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====conversation====\n",
      " [{'role': 'user', 'content': '你好'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/gradio/queueing.py\", line 522, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/gradio/blocks.py\", line 1741, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/gradio/blocks.py\", line 1308, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/gradio/utils.py\", line 575, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/gradio/utils.py\", line 568, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/gradio/utils.py\", line 551, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/gradio/utils.py\", line 734, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "  File \"/tmp/ipykernel_17087/781023984.py\", line 32, in predict\n",
      "    for new_token in streamer:\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/transformers/generation/streamers.py\", line 223, in __next__\n",
      "    value = self.text_queue.get(timeout=self.timeout)\n",
      "  File \"/root/miniconda3/lib/python3.10/queue.py\", line 179, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n"
     ]
    }
   ],
   "source": [
    "demo.launch(server_name=\"127.0.0.1\", server_port=7870, inbrowser=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c8ccd-405a-498c-96fd-8f75b5ef9c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-03 10:32:47--  https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64\n",
      "Resolving cdn-media.huggingface.co (cdn-media.huggingface.co)... 31.13.76.65, 2600:9000:2363:b400:19:6fb8:2ac0:93a1, 2600:9000:2363:7a00:19:6fb8:2ac0:93a1, ...\n",
      "Connecting to cdn-media.huggingface.co (cdn-media.huggingface.co)|31.13.76.65|:443... "
     ]
    }
   ],
   "source": [
    "!wget \"https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64\" -O /root/miniconda3/lib/python3.10/site-packages/gradio/frpc_linux_amd64_v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecae382-181f-4ef0-88e9-b1c1642a7454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载失败，手动上传"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed1e25b-0f22-4568-93e5-4f9b4b0fea9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root 1.0M Apr  3 10:34 frpc_linux_amd64\n"
     ]
    }
   ],
   "source": [
    "ll -h frpc_linux_amd64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bad6aa3-b547-4d53-9789-e1f1839f540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv frpc_linux_amd64 frpc_linux_amd64_v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e8216b4-21f6-494d-842b-c9cfc3ff1978",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv frpc_linux_amd64_v0.2 /root/miniconda3/lib/python3.10/site-packages/gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceff894b-9c04-46c9-9964-820b2a510e68",
   "metadata": {},
   "source": [
    "# 整合后的web_demo_gradio.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f61c36d-0e06-4cf9-841b-7a8aece20843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d992472e5241cca22e3df010f155ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====conversation====\n",
      " [{'role': 'user', 'content': '你好'}]\n",
      "\n",
      "\n",
      "====conversation====\n",
      " [{'role': 'user', 'content': '你好'}, {'role': 'assistant', 'content': '你好，我是人工智能助手。有什么我可以帮助你的吗？'}, {'role': 'user', 'content': '类型#裙*裙长#半身裙'}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import torch\n",
    "from threading import Thread\n",
    "\n",
    "from typing import Union, Annotated\n",
    "from pathlib import Path\n",
    "from peft import AutoPeftModelForCausalLM, PeftModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerFast,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    TextIteratorStreamer\n",
    ")\n",
    "\n",
    "ModelType = Union[PreTrainedModel, PeftModelForCausalLM]\n",
    "TokenizerType = Union[PreTrainedTokenizer, PreTrainedTokenizerFast]\n",
    "\n",
    "# MODEL_PATH = '/root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b'\n",
    "# TOKENIZER_PATH = '/root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b'\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(\n",
    "        model_dir: Union[str, Path], trust_remote_code: bool = True\n",
    ") -> tuple[ModelType, TokenizerType]:\n",
    "    model_dir = _resolve_path(model_dir)\n",
    "    if (model_dir / 'adapter_config.json').exists():\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            model_dir, trust_remote_code=trust_remote_code, device_map='auto'\n",
    "        )\n",
    "        tokenizer_dir = model.peft_config['default'].base_model_name_or_path\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir, trust_remote_code=trust_remote_code, device_map='auto'\n",
    "        )\n",
    "        tokenizer_dir = model_dir\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_dir, trust_remote_code=trust_remote_code\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "model_dir = 'output/checkpoint-2000' # 输入微调后的Checkpoint目录地址\n",
    "model, tokenizer = load_model_and_tokenizer(model_dir, trust_remote_code=True)\n",
    "\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [0, 2]\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def parse_text(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if line != \"\"]\n",
    "    count = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"```\" in line:\n",
    "            count += 1\n",
    "            items = line.split('`')\n",
    "            if count % 2 == 1:\n",
    "                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n",
    "            else:\n",
    "                lines[i] = f'<br></code></pre>'\n",
    "        else:\n",
    "            if i > 0:\n",
    "                if count % 2 == 1:\n",
    "                    line = line.replace(\"`\", \"\\`\")\n",
    "                    line = line.replace(\"<\", \"&lt;\")\n",
    "                    line = line.replace(\">\", \"&gt;\")\n",
    "                    line = line.replace(\" \", \"&nbsp;\")\n",
    "                    line = line.replace(\"*\", \"&ast;\")\n",
    "                    line = line.replace(\"_\", \"&lowbar;\")\n",
    "                    line = line.replace(\"-\", \"&#45;\")\n",
    "                    line = line.replace(\".\", \"&#46;\")\n",
    "                    line = line.replace(\"!\", \"&#33;\")\n",
    "                    line = line.replace(\"(\", \"&#40;\")\n",
    "                    line = line.replace(\")\", \"&#41;\")\n",
    "                    line = line.replace(\"$\", \"&#36;\")\n",
    "                lines[i] = \"<br>\" + line\n",
    "    text = \"\".join(lines)\n",
    "    return text\n",
    "\n",
    "\n",
    "def predict(history, max_length, top_p, temperature):\n",
    "    stop = StopOnTokens()\n",
    "    messages = []\n",
    "    for idx, (user_msg, model_msg) in enumerate(history):\n",
    "        if idx == len(history) - 1 and not model_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            break\n",
    "        if user_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        if model_msg:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": model_msg})\n",
    "\n",
    "    print(\"\\n\\n====conversation====\\n\", messages)\n",
    "    model_inputs = tokenizer.apply_chat_template(messages,\n",
    "                                                 add_generation_prompt=True,\n",
    "                                                 tokenize=True,\n",
    "                                                 return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=60, skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = {\n",
    "        \"input_ids\": model_inputs,\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": max_length,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": top_p,\n",
    "        \"temperature\": temperature,\n",
    "        \"stopping_criteria\": StoppingCriteriaList([stop]),\n",
    "        \"repetition_penalty\": 1.2,\n",
    "    }\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    for new_token in streamer:\n",
    "        if new_token != '':\n",
    "            history[-1][1] += new_token\n",
    "            yield history\n",
    "\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"\"\"<h1 align=\"center\">ChatGLM3-6B Gradio Simple Demo</h1>\"\"\")\n",
    "    chatbot = gr.Chatbot()\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            with gr.Column(scale=12):\n",
    "                user_input = gr.Textbox(show_label=False, placeholder=\"Input...\", lines=10, container=False)\n",
    "            with gr.Column(min_width=32, scale=1):\n",
    "                submitBtn = gr.Button(\"Submit\")\n",
    "        with gr.Column(scale=1):\n",
    "            emptyBtn = gr.Button(\"Clear History\")\n",
    "            max_length = gr.Slider(0, 32768, value=8192, step=1.0, label=\"Maximum length\", interactive=True)\n",
    "            top_p = gr.Slider(0, 1, value=0.8, step=0.01, label=\"Top P\", interactive=True)\n",
    "            temperature = gr.Slider(0.01, 1, value=0.6, step=0.01, label=\"Temperature\", interactive=True)\n",
    "\n",
    "\n",
    "    def user(query, history):\n",
    "        return \"\", history + [[parse_text(query), \"\"]]\n",
    "\n",
    "\n",
    "    submitBtn.click(user, [user_input, chatbot], [user_input, chatbot], queue=False).then(\n",
    "        predict, [chatbot, max_length, top_p, temperature], chatbot\n",
    "    )\n",
    "    emptyBtn.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.queue()\n",
    "demo.launch(server_name=\"127.0.0.1\", server_port=7870, inbrowser=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
