# 前言

> 在极客时间AI大模型微调训练营的学习经历让我实现了对AI趋势的深刻理解和微调技术的实践应用。作为AI业务落地和产品平台提效的专业人员，我通过这门课程深化了对AI技术发展浪潮的认识，掌握了大模型的核心技术，如提示工程、微调、预训练技术等，以及如何处理数据集和定位业务目标。实战练习，特别是使用Hugging Face Transformers和LangChain等工具的应用，极大地提升了我的实践能力，让我能够独立完成微调任务的分析、验证和应用落地，有效提升了工作效率并支持了公司项目的实施。

在极客时间 AI 大模型微调训练营学习一学期下来，目前就要结束了，现对于整个学期做个总结如下：

## 1、目前在什么岗位做什么？

目前在做

1、AI业务场景落地

2、提效AI开发的产品平台。

## 2、为什么选择报名学习此训练营？

- 对AI趋势预判、极大的感兴趣

- 希望通过训练营入门AI，掌握概念、操作，能够在学完之后可以自行继续深入研究。
- 提高对AI大模型微调的认知和实践，个人长期投入以及公司项目需要。

事实证明，以上目标均已达到。

## 3、课程讲的如何？这个课程你的收获是什么？

课程讲的由浅入深，更多的都是理论性知识、论文相关的技术原理性讲解。

辅助以一些google colab的notebook操作手册直接上手实践，学中做，做中学。

收获有很多：

通过这门课程，我深刻理解了AI技术发展的四轮浪潮，包括从弱人工智能到机器学习、深度学习乃至大语言模型的演进，以及这些技术在全球范围内如何引发高校共识、硅谷创新乃至中美博弈。特别是，我掌握了如何利用AI大模型助力个体和小团队的实战能力，这对我未来的学习和工作都有着不可估量的价值。

我学会了大模型的核心技术，包括提示工程、AI智能体、大模型微调和预训练技术。我还深入了解了统计语言模型、神经网络语言模型、基于Transformer的大语言模型、注意力机制等先进技术，这让我能够更深入地理解大模型背后的原理和架构。

通过Hugging Face Transformers的快速入门，我不仅掌握了Transformers库的核心功能，还通过实战练习掌握了如何使用Pipelines、Tokenizer和Models等工具，以及如何在Google Colab等平台上搭建GPU开发环境进行模型训练和微调。我还学习了如何处理数据集、进行模型训练评估，并且通过实战项目深化了对Text Classification和Question Answering任务的理解。

我对大模型的高效微调（PEFT）技术和低秩适配（LoRA）技术有了全面的了解，包括Adapter Tuning、Prefix Tuning、Prompt Tuning等技术的应用，这些都极大地提升了我的技术实践能力。

我还通过LangChain学习了大模型应用开发框架的基础知识，理解了大模型抽象、最佳实践和数据处理流等概念，为我未来开发和部署AI应用奠定了坚实的基础。

此外，我通过实战项目深入了解了智谱AI GLM大模型家族、Meta LLaMA大模型技术、ChatGPT的训练核心技术、基于人类反馈的强化学习（RLHF）技术以及混合专家模型（MoEs）技术等前沿技术，这些知识让我对AI技术的未来发展趋势有了更清晰的认识。

总之，这门课程不仅让我掌握了AI领域的理论知识和技术原理，更重要的是，通过大量的实战练习，我获得了将理论应用到实践中的能力，这对我的职业生涯是一笔宝贵的财富。



课程目录总结：

- 深度解读 AI 发展四轮浪潮

   技术浪潮：弱人工智能、机器学习、深度学习、大语言模型

   应用浪潮：高校共识、硅谷创新、中美博弈

   把握浪潮：AI 大模型助力超级个体和小团队

- AI 大模型四阶技术总览

   提示工程（Prompt Engineering）

   AI智能体（Agents）

   大模型微调（Fine-tuning）

   预训练技术（Pre-training）
  
- 统计语言模型
 神经网络语言模型
  基于Transformer的大语言模型
  注意力机制
  Transformer 网络架构
  预训练 Transformer 模型: GPT-1 与 BERT
  暴力美学 GPT 系列模型

- 大模型高效微调（PEFT）技术
 Adapter Tuning (2019 Google)
  Prefix Tuning (2021 Stanford)
  Prompt Tuning (2021 Google)
  P-Tuning v1 (2021 Tsinghua, MIT)
  P-Tuning v2 (2022 Tsinghua, BAAI, Shanghai Qi Zhi Institute)
- 大模型低秩适配（LoRA）技术
 LoRA (2021 Microsoft)
  AdaLoRA ( 2023 Microsoft, Princeton, Georgia Tech)
  QLoRA (2023 University of Washington)
- 大模型高效微调（PEFT）未来发展趋势

- Hugging Face Transformers 快速入门
 Transformers 库是什么？
  Transformers 核心功能模块
  使用 Pipelines 快速实践大模型
  使用 Tokenizer 编解码文本
  使用 Models 加载和保存模型
- 大模型开发环境搭建
 搭建你的 GPU 开发环境
  Google Colab 测试环境
  实战 Hugging Face Transformers 工具库

- 数据集处理库 Hugging Face Datasets
 Hugging Face Datasets 库简介
  数据预处理策略：填充与截断
  使用 Datasets.map 方法处理数据集
- Transformers 微调训练模块 Trainer
 基础微调训练类Trainer
  训练参数与超参数配置 TrainingArguments
- 模型训练评估库 Hugging Face Evaluate
- 实战 Transformers 微调 bert-base-cased 模型（文本分类任务）
- 实战 Transformers 微调 distilbert-base-uncased 模型（QA 任务）

- Transformers 核心设计 Auto Classes
 灵活扩展的配置 AutoConfig
  自动化模型管理 AutoModel
  通用分词器 AutoTokenizer
- Transformers 模型量化 Quantization
 模型参数与显存占用计算基础
  GPTQ 面向 Transformer 模型量化技术
  激活感知权重量化（AWQ）算法
  使用 BitsAndBytes （bnb）快速实现参数精度量化
- 实战 Transformers 模型量化 Facebook OPT

- Hugging Face PEFT 快速入门
   PEFT 库是什么？
   PEFT 与 Transformers 结合
   PEFT 核心类定义与功能说明
     AutoPeftModels、PeftModel
     PeftConfig
     PeftType | TaskType
- 实战 PEFT 库模型微调多种下游任务
 LoRA 低秩适配 OPT-6.7B 文本生成任务
  LoRA 低秩适配 OpenAI Whisper-Large-V2 语音识别任务

- 智谱AI GLM 大模型家族
     最强基座模型 GLM-130B
     联网检索能力 WebGLM
     初探多模态 VisualGLM-6B
     多模态预训练模型 CogVLM
     代码生成模型 CodeGeex2
     增强对话能力 ChatGLM
- 实战 QLoRA 微调 ChatGLM3-6B 大模型


- 大模型应用开发框架 LangChain 简介
     LangChain 是什么
     为什么需要 LangChain
     LangChain 典型使用场景
     LangChain 基础概念与模块化设计

- LangChain 核心模块入门与实战
     标准化的大模型抽象：Mode I/O
     大模型应用的最佳实践：Chains
     赋予应用记忆的能力：Memory
     框架原生的数据处理流：Data Connection
    
- ChatGLM3-6B 模型私有化部署
     使用 Transformers 库加载模型
     使用 Gradio WebUI 启动模型（Basic Demo）
     使用 Streamlit WebUI 启动模型（Composite Demo）
     使用类 OpenAI API 形式部署模型（Chat Mode）
- LangChain 与 ChatGLM 生态集成
     使用 LangChain 调用 ZHIPU AI 模型服务（API KEY）
     私有化部署 ChatGLM2-6B API Server（LLM Mode）

- 实战：基于 LangChain 和 ChatGLM 私有化部署聊天机器人
     使用 LLMChain 调用私有化 ChatGLM2-6B 模型服务
     使用 ConversationChain 实现 ChatGLM2-6B 连续对话
     使用 Gradio WebUI 启动聊天机器人
    
- 实战构造私有的微调数据集
     基于 ChatGPT 设计生成训练数据的 Prompt
     使用 LangChain + GPT-3.5-Turbo 生成训练数据样例
     训练数据解析、数据增强和持久化存储
     自动化批量生成训练数据集流水线
- 实战私有数据微调 ChatGLM3
     使用 QLoRA 小样本微调 ChatGLM3
     ChatGLM3 微调前后效果对比
     大模型训练过程分析与数据优化
    
- 实战构造私有的微调数据集
     基于 ChatGPT 设计生成训练数据的 Prompt
     使用 LangChain + GPT-3.5-Turbo 生成训练数据样例
     训练数据解析、数据增强和持久化存储
     自动化批量生成训练数据集流水线
- 实战私有数据微调 ChatGLM3
     使用 QLoRA 小样本微调 ChatGLM3
     ChatGLM3 微调前后效果对比
     大模型训练过程分析与数据优化


- Meta LLaMA 1 大模型技术解读
     基座模型系列：LLaMA 1-7B(, 13B, 33B, 65B)
     LLaMA 1 改进网络架构和预训练方法
     LLaMA 1 衍生模型大家族

- Meta LLaMA 2 大模型技术解读
     基座模型系列： LLaMA 2-7B(, 13B, 70B)
     指令微调模型：LLaMA 2-Chat
     如何获取 LLaMA-2 模型预训练权重

- ChatGPT 大模型训练核心技术
     阶段一：万亿级 Token 预训练语言模型
     阶段二：有监督指令微调（SFT）语言模型
     阶段三：使用 RLHF 实现人类价值观对齐（Alignment）

- 基于人类反馈的强化学习（RLHF）技术详解
     步骤一：使用SFT微调预训练语言模型
     步骤二：训练奖励模型（Reward Model）
     步骤三：使用 PPO 优化微调语言模型

- 混合专家模型（Mixture-of-Experts, MoEs）技术发展简史
     开山鼻祖：自适应局部专家混合（ Michael I. Jordan & Hinton, 1991）
     多层次混合：深度MoEs中的表示学习（ Ilya, 2013）
     稀疏门控：支持超大网络的MoEs（Hinton & Jeff Dean, 2017）
- MoEs 与 大模型结合后的技术发展
     GShard：基于 MoE 探索巨型 Transformer 网络（Google, 2020）
     GLaM：使用 MoE 扩展语言模型性能（Google, 2021）
     Switch Transformer：使用稀疏技术实现万亿模型（Google, 2022）
     MoEs 实例研究：Mixtral-8x7B-v0.1（Mistral AI, 2023）

- 大模型分布式训练框架 DeepSpeed
     Zero Redundancy Optimizer (ZeRO) 技术
     DeepSpeed 框架和核心技术
     分布式模型训练并行化技术对比
     DeepSpeed 与 Transformers 集成训练大模型
     DeepSpeed ZeRO 配置详解
     使用 DeepSpeed 单机多卡、分布式训练
- 实战 DeepSpeed ZeRO-2 和 ZeRO-3 单机单卡训练
- DeepSpeed 创新模块：Inference、Compression & Science

## 4、学完这个课程有没有解决工作中的关键问题

> 比如是靠学到的知识找到了新的工作，还是说在公司内部做成了一些项目，得到了认可之类的

通过参与AI大模型微调训练营，我现在能够更有效地与AI微调团队进行沟通，准确判断哪些目标可实现，哪些暂时无法达成，并且我了解了实现的原理。我学会了如何准备、处理和制作数据集，以及如何明确业务和技术目标，确保项目的成功实施。

此外，我不仅掌握了大模型微调的理论知识，还获得了丰富的实践经验。通过结合最新的技术工具，如LangChain、各种开源框架和模型，我现在能够独立完成微调任务的分析、验证和实际应用。

这一过程中，我积累了大量的大模型微调知识和经验，这些技能极大地提高了我在工作中处理AI大模型训练和数据处理的效率。更重要的是，这些技能为我构建更高效、更强大的AI平台提供了坚实的基础。

## 5、对这个课程有什么意见建议？

对于这门课程，我的建议主要集中在以下几个方面：

1. **实战项目增多**：虽然课程已经通过Google Colab的notebook提供了不少实践机会，但我建议可以增加更多的实战项目，尤其是跨领域的应用案例。通过解决来自不同行业的实际问题，学生可以更好地理解AI技术的广泛应用，同时也能够锻炼解决复杂问题的能力。
2. **更新最新研究和技术动态**：AI领域发展迅速，新的研究成果和技术不断涌现。课程内容应当定期更新，包括最新的大模型研究成果、技术突破以及行业应用案例，以保证学生能够接触到最前沿的知识和技术。
3. **加强社区和团队协作学习**：建议课程中增加更多的团队协作项目，鼓励学生在学习过程中形成小组，共同解决问题。这不仅能够提高学习效率，还能够培养学生的团队协作能力和沟通能力。同时，可以考虑建立一个学习社区，为学生提供交流和分享经验的平台。
4. **强化理论与实践的联系**：虽然课程已经做到了理论联系实际，但仍可以进一步强化这一点。例如，可以在讲解理论知识后，立即通过相关的实战项目来应用这些理论，帮助学生更好地理解和掌握。
5. **提供个性化学习路径**：鉴于学生的背景和兴趣可能有所不同，课程可以考虑提供更多的选修内容和个性化学习路径。例如，对于想深入研究某一特定领域（如自然语言处理、计算机视觉等）的学生，可以提供专门的深度课程或项目。
6. **加强评估和反馈机制**：建议增设更多形式多样的评估方式，如项目评审、同行评价等，以及及时反馈的机制，帮助学生了解自己的学习进度和存在的问题，从而更有针对性地改进和学习。